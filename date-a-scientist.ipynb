{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import re\n",
    "from part_of_speech import get_part_of_speech\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['age', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'essay0',\n",
       "       'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7',\n",
       "       'essay8', 'essay9', 'ethnicity', 'height', 'income', 'job',\n",
       "       'last_online', 'location', 'offspring', 'orientation', 'pets',\n",
       "       'religion', 'sex', 'sign', 'smokes', 'speaks', 'status'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "df = pd.read_csv('../okcupiddata/profiles.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-76-dfd803fa42db>:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_essays['corpus'] = df_essays[df_essays.columns[0:]].apply(\n"
     ]
    }
   ],
   "source": [
    "#make dataframe just containing essays\n",
    "df_essays = df[['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9']]\n",
    "df_essays['corpus'] = df_essays[df_essays.columns[0:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1)\n",
    "corpus = df_essays['corpus'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "    #remove html tags\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    text = re.sub(clean, '', text)\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    #remove newline, tab\n",
    "    text = re.sub(r'\\r+|\\n+|\\t+',' ',text)\n",
    "    return text\n",
    "\n",
    "def tokenize_lemmatize_remove_stop(text):\n",
    "    tokenized = word_tokenize(str(text))\n",
    "    #print(len(tokenized))\n",
    "    lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    no_stop = [word for word in lemmatized if word not in stop_words]\n",
    "    return no_stop\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['would', 'love', 'think', 'kind', 'intellectual', 'either', 'dumbest', 'smart', 'guy', 'smart', 'dumb', 'guy', 'cant', 'say', 'tell', 'difference', 'love', 'talk', 'idea', 'concept', 'forge', 'odd', 'metaphor', 'instead', 'recite', 'cliche', 'like', 'simularities', 'friend', 'mine', 'house', 'underwater', 'salt', 'mine', 'favorite', 'word', 'salt', 'way', 'weird', 'choice', 'know', 'thing', 'life', 'well', 'metaphor', 'seek', 'make', 'little', 'well', 'everyday', 'productively', 'lazy', 'way', 'get', 'tire', 'tie', 'shoe', 'consider', 'hire', 'five', 'year', 'old', 'would', 'probably', 'tie', 'shoe', 'decide', 'wear', 'leather', 'shoe', 'dress', 'shoe', 'love', 'really', 'serious', 'really', 'deep', 'conversation', 'really', 'silly', 'stuff', 'snap', 'light', 'hearted', 'rant', 'kiss', 'dont', 'funny', 'able', 'make', 'laugh', 'able', 'bend', 'spoon', 'mind', 'telepathically', 'make', 'smile', 'still', 'work', 'love', 'life', 'cool', 'let', 'wind', 'blow', 'extra', 'point', 'read', 'guess', 'favorite', 'video', 'game', 'hint', 'give', 'yet', 'lastly', 'good', 'attention', 'spancurrently', 'work', 'international', 'agent', 'freight', 'forwarding', 'company', 'import', 'export', 'domestic', 'know', 'work', 'online', 'class', 'try', 'well', 'free', 'time', 'perhaps', 'hour', 'worth', 'good', 'book', 'video', 'game', 'lazy', 'sundaymaking', 'people', 'laugh', 'ranting', 'good', 'salt', 'find', 'simplicity', 'complexity', 'complexity', 'simplicitythe', 'way', 'look', 'six', 'foot', 'half', 'asian', 'half', 'caucasian', 'mutt', 'make', 'tough', 'notice', 'blend', 'inbooks', 'absurdistan', 'republic', 'mouse', 'men', 'book', 'make', 'want', 'cry', 'catcher', 'rye', 'prince', 'movie', 'gladiator', 'operation', 'valkyrie', 'producer', 'periscope', 'show', 'borgia', 'arrest', 'development', 'game', 'throne', 'monty', 'python', 'music', 'aesop', 'rock', 'hail', 'mary', 'mallon', 'george', 'thorogood', 'delaware', 'destroyer', 'felt', 'food', 'im', 'anythingfood', 'water', 'cell', 'phone', 'shelterduality', 'humorous', 'thingstrying', 'find', 'someone', 'hang', 'anything', 'except', 'clubi', 'new', 'california', 'look', 'someone', 'wisper', 'secret', 'toyou', 'want', 'sweep', 'foot', 'tire', 'norm', 'want', 'catch', 'coffee', 'bite', 'want', 'talk', 'philosophy']\n"
     ]
    }
   ],
   "source": [
    "denoised = [remove_noise(str(i)) for i in corpus]\n",
    "normalized = [tokenize_lemmatize_remove_stop(i) for i in denoised]\n",
    "print(normalized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize and fit TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(denoised)\n",
    "\n",
    "tfidf_norm = normalize(tfidf)\n",
    "tfidf_array = tfidf_norm.toarray()\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_array, columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    00  000  000000001  000000002  000000bubbles  00001  000you  001  002  \\\n0  0.0  0.0        0.0        0.0            0.0    0.0     0.0  0.0  0.0   \n1  0.0  0.0        0.0        0.0            0.0    0.0     0.0  0.0  0.0   \n2  0.0  0.0        0.0        0.0            0.0    0.0     0.0  0.0  0.0   \n3  0.0  0.0        0.0        0.0            0.0    0.0     0.0  0.0  0.0   \n4  0.0  0.0        0.0        0.0            0.0    0.0     0.0  0.0  0.0   \n\n   003  ...  zzzzzz  zzzzzzz  zzzzzzzits  zzzzzzzz  zzzzzzzzz  zzzzzzzzzz  \\\n0  0.0  ...     0.0      0.0         0.0       0.0        0.0         0.0   \n1  0.0  ...     0.0      0.0         0.0       0.0        0.0         0.0   \n2  0.0  ...     0.0      0.0         0.0       0.0        0.0         0.0   \n3  0.0  ...     0.0      0.0         0.0       0.0        0.0         0.0   \n4  0.0  ...     0.0      0.0         0.0       0.0        0.0         0.0   \n\n   zzzzzzzzzzingmy  zzzzzzzzzzs  zzzzzzzzzzzzshow  zzzzzzzzzzzzzzzzdigital  \n0              0.0          0.0               0.0                      0.0  \n1              0.0          0.0               0.0                      0.0  \n2              0.0          0.0               0.0                      0.0  \n3              0.0          0.0               0.0                      0.0  \n4              0.0          0.0               0.0                      0.0  \n\n[5 rows x 480474 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    \"\"\" K Means Clustering\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        k: int , number of clusters\n",
    "        \n",
    "        seed: int, will be randomly set if None\n",
    "        \n",
    "        max_iter: int, number of iterations to run algorithm, default: 200\n",
    "        \n",
    "    Attributes\n",
    "    -----------\n",
    "       centroids: array, k, number_features\n",
    "       \n",
    "       cluster_labels: label for each data point\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k, seed = None, max_iter = 200):\n",
    "        self.k = k\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "            \n",
    "    \n",
    "    def initialise_centroids(self, data):\n",
    "        \"\"\"Randomly Initialise Centroids\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        centroids: array of k centroids chosen as random data points \n",
    "        \"\"\"\n",
    "        \n",
    "        initial_centroids = np.random.permutation(data.shape[0])[:self.k]\n",
    "        self.centroids = data[initial_centroids]\n",
    "\n",
    "        return self.centroids\n",
    "    \n",
    "    \n",
    "    def assign_clusters(self, data):\n",
    "        \"\"\"Compute distance of data from clusters and assign data point\n",
    "           to closest cluster.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        cluster_labels: index which minmises the distance of data to each\n",
    "        cluster\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        if data.ndim == 1:\n",
    "            data = data.reshape(-1, 1)\n",
    "        \n",
    "        dist_to_centroid =  pairwise_distances(data, self.centroids, metric = 'euclidean')\n",
    "        self.cluster_labels = np.argmin(dist_to_centroid, axis = 1)\n",
    "        \n",
    "        return  self.cluster_labels\n",
    "    \n",
    "    \n",
    "    def update_centroids(self, data):\n",
    "        \"\"\"Computes average of all data points in cluster and\n",
    "           assigns new centroids as average of data points\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        centroids: array, k, number_features\n",
    "        \"\"\"\n",
    "        \n",
    "        self.centroids = np.array([data[self.cluster_labels == i].mean(axis = 0) for i in range(self.k)])\n",
    "        \n",
    "        return self.centroids\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Predict which cluster data point belongs to\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        cluster_labels: index which minmises the distance of data to each\n",
    "        cluster\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.assign_clusters(data)\n",
    "    \n",
    "    def fit_kmeans(self, data):\n",
    "        \"\"\"\n",
    "        This function contains the main loop to fit the algorithm\n",
    "        Implements initialise centroids and update_centroids\n",
    "        according to max_iter\n",
    "        -----------------------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        instance of kmeans class\n",
    "            \n",
    "        \"\"\"\n",
    "        self.centroids = self.initialise_centroids(data)\n",
    "        \n",
    "        # Main kmeans loop\n",
    "        for iter in range(self.max_iter):\n",
    "\n",
    "            self.cluster_labels = self.assign_clusters(data)\n",
    "            self.centroids = self.update_centroids(data)          \n",
    "            if iter % 100 == 0:\n",
    "                print(\"Running Model Iteration %d \" %iter)\n",
    "        print(\"Model finished running\")\n",
    "        return self   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Y_sklearn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-0d2d4839b903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_sklearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_sklearn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-0d2d4839b903>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_sklearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_sklearn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "number_clusters = range(1, 7)\n",
    "\n",
    "kmeans = [KMeans(n_clusters=i, max_iter = 600) for i in number_clusters]\n",
    "kmeans\n",
    "\n",
    "score = [kmeans[i].fit(Y_sklearn).score(Y_sklearn) for i in range(len(kmeans))]\n",
    "score\n",
    "\n",
    "plt.plot(number_clusters, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}